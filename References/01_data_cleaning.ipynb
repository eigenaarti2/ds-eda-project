{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a Data Scientist, you will spent most time on data cleaning. It is a process which can be annoying some times and definitely one you have to do over and over and over again. But it is also one of the most important steps of the data science lifecycle since it determines the quality of your data. Remember the \"garbage in, garbage out\" concept! Therefore it has a huge impact on your prediction model later on and is time well spent. Most of the data you will see in this course is already pretty much cleaned up for modelling. In a business context, this is usually not the case.\n",
    "\n",
    "In this notebook we will learn a few tools which can be helpful while cleaning your data. Also we will learn some techniques how to handle missing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this notebook you will be able to:\n",
    "\n",
    "* inspect your data and use the general pandas function for the first analyses\n",
    "* handle missing data and use some imputing techniques\n",
    "* know when to drop data\n",
    "* transform your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it gets probably usual by now, we import our most needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from `seattle-weather_raw.csv` into a pandas data frame. You might already know this dataset from the pandas exercises notebooks. We gave you the dataset preprocessed. Now it's the raw file and we need to give it some thought to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing raw data\n",
    "df_weather = pd.read_csv('data/seattle-weather_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:**\n",
    ">\n",
    "> Get a quick overview of the data. Use the learned pandas methods like `.head()`, `.tail()`, `.info()` and `.describe()`. Is there anything you notice? What would you address in the data cleaning process?\n",
    "\n",
    "<details><summary>\n",
    "Click here for a hint…\n",
    "</summary>\n",
    "Do you see some:\n",
    "  \n",
    "    - formatting problems in the column names?\n",
    "    - duplicates? \n",
    "    - missing values? (what is the reason? and what can you do about it?)\n",
    "    - inconsistency in the meaning of data?\n",
    "    - different unit for data?\n",
    "    - different data types than expected?\n",
    "    - obvious outliers/ unexpected values?\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code: check for data types and statistics overview of data frame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.head(7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix column names formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you are probably aware of, we have to change the column names. As already explained in earlier notebooks, it is always a good idea to change the empty spaces to underscores. Also it is recommended to use only low letters in the column names. Furthermore the last column has no name.\n",
    "\n",
    "One way to change the names is to write the actual names into a list, editing them and transfer the list back to the data frame, as it is explained in the pandas notebooks. But it is also possible to edit the column names in the data frame directly. Of course in the end you can use the way which you feel is the most comfortable.\n",
    "\n",
    "The `.str` function allows you to use string functions on a data frame column. We can use that to simply replace empty spaces with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing empty spaces with underscores in column names\n",
    "df_weather.columns = df_weather.columns.str.replace(' ','_')\n",
    "df_weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we can just simply tell our data frame to only use lower cases for our column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all column names to lower case\n",
    "df_weather.columns = df_weather.columns.str.lower()\n",
    "df_weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also could use `.str` for renaming the last column name. But pandas has already the `.rename` function build in. That way we can iterate directly into the column names and only need a dictionary to change the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming last column\n",
    "df_weather = df_weather.rename({'-':'weather'},axis=1)\n",
    "df_weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations. You just finished the first step of cleaning this data frame. Let's have another look at the first rows again. What else can you see which is odd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with duplicates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows with index $2$ and $3$ are identical. There are obviously duplicated which can happen for example while merging two data frames together. The function `.duplicated()` returns a boolean pandas Series which tells you if a row is a duplicate or not. Combined with another well known function it will tell us how many duplicated rows we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many duplicated rows exist in the data frame\n",
    "df_weather.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have 37 duplicated rows. Those rows can be easily dropped. Furthermore we have to drop the old index and renew them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_weather = df_weather.drop_duplicates()\n",
    "# reset index inplace\n",
    "df_weather.reset_index(inplace=True, drop=True)\n",
    "df_weather.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types and Transforming Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Part of Data Cleaning is to get the data types into the right shape. This is import so we can work better with the data later on. The dtype `object` means that there are several data types in the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types in data frame\n",
    "df_weather.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the column list and start at the beginning with the **Date** column. At the moment it has the data type `Object`. We will look at this more closely to see what data type Pandas identifies our current date entry as. To do this, we can simply take the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of first date entry\n",
    "type(df_weather['date'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our date entry is just a string. We can change that to a date time very easy as we already learned. Note that you also could do that in the beginning, when you read in the csv file with the parameter `parse_date=['date']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"date\" dtype to datetime with format %Y/%m/%d\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'], format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check if that worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \"date\" dtype\n",
    "type(df_weather['date'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Moving on to the column **precipitation**. At the look of the data frames head, we would have expected a float datatype. This is actually an indication, that there is more than just numbers in this column. For now we could just try to convert the column to the float type, also this might not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data type to float\n",
    "df_weather = df_weather.astype({'precipitation': float})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we get an error message. However this can help us to determine why our column cannot be assigned to the data type `float`. If we read the message it tells us in the last line that we can not convert a string to a float. Also now we know the cause of our error. Somewhere in this column is a `$`-character hidden. Out of curiosity let's try to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display rows with \"$\" in precipitation column\n",
    "df_weather.query('precipitation == \"$\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is only one `$`-character in line $1130$. We can easily replace the `$`-character with a numpy NaN value. NaN stands for \"Not a Number\" and is used as a placeholder for missing values. Afterwards we are able to convert our column data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the `$`-character with a numpy NaN value\n",
    "df_weather['precipitation'] = df_weather.precipitation.replace('$',np.NaN)\n",
    "# change data type to float\n",
    "df_weather = df_weather.astype({'precipitation': float})\n",
    "df_weather.precipitation.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our data types and the table head again, the `temp_max`-column looks good. But if we continue with `temp_min` we will notice two things. First there are not only numbers in `temp_min` (dtype is detected as object) and the minimum temperature is larger as the maximum temperature. We can assume that somehow the units of the temperature got mixed up. The maximum temperature is in Celsius while the minimum is appearing to be in Fahrenheit. To fix that we can first delete the unit from the number and change the data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unit from \"temp_min\" column, change dtype to float\n",
    "df_weather['temp_min'] = df_weather.temp_min.str.strip(' F').astype('float')\n",
    "df_weather.temp_min.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have floats in our column we can convert the temperature from Fahrenheit to Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert temp_min to celsius\n",
    "df_weather['temp_min'] = df_weather.temp_min.apply(lambda x: (x-32)/1.8)\n",
    "df_weather.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is much better. Also the minimum temperature is now smaller than the maximum temperature.\n",
    "\n",
    "For the \"Wind\" column, the data type is already as it should be. However, you should round these numbers for the future. It might be good to have a few decimal places for future modelling, but it is not always necessary to be so precise. Let's therefor try rounding these numbers for practice purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounding wind data\n",
    "df_weather['wind'] = df_weather.wind.round(2)\n",
    "df_weather.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last column doesn't contain a number but a string, which describes the type of weather. That kind of data is called a categorical value. We can find out how many categorical values there are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display number of distinct elements\n",
    "df_weather.weather.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is quite a lot, but also as you may noticed not accurate. We have various entries which mean the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display all distinct elements\n",
    "df_weather['weather'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the unique values, we see that \"drizzle\", \"d\", \"d.\" and \"drizzle\" all probably represent the same category. So one category has several values. This often happens if there is no fixed standard for data collection. We need to reduce the number of values for each category to one. The easiest and safest way is to manually assign each value to the appropriate category. The implemented function `.map` uses either a dictionary to assign a value to it's new one or a function, which is applied on each value. In our case we will use the dictionary.\n",
    "\n",
    "> Note: Sometimes it can happen that abbreviations are ambiguous. For example, there is a category \"sn\" which could represent either \"snow\" or \"sun\". Here it is worth diving deeper into the data collection process to make a more informed decision.\n",
    "\n",
    "> <sup>Note: you could also use pattern match (which uses regular expressions) or fuzzy matching to approach this task. But they are more complicated to program and an absolute understanding of them is mandatory, because there is a risk of assigning a value to the wrong category.</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing number of categorical values by mapping with a word-dictionary \n",
    "df_weather['weather'] = df_weather['weather'].map({\n",
    "    'drizzle': 'drizzle',\n",
    "    'r': 'rain', \n",
    "    'r.': 'rain',\n",
    "    'rain' : 'rain',\n",
    "    's': 'sun',\n",
    "    'Rain': 'rain',  \n",
    "    'sun': 'sun',\n",
    "    'Sun': 'sun',\n",
    "    'Snow': 'snow',\n",
    "    'sn.': 'snow', \n",
    "    'sw': 'snow',\n",
    "    'd': 'drizzle', \n",
    "    's.': 'sun',\n",
    "    'driz.': 'drizzle', \n",
    "    'snow': 'snow',\n",
    "    'Drizzle': 'drizzle', \n",
    "    'Fog': 'fog',\n",
    "    'f': 'fog',\n",
    "    'f.': 'fog', \n",
    "    'fog': 'fog'\n",
    "    })\n",
    "df_weather.weather.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the data type from this column to `string` just as easily as before. Note that this is not mandatory going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dtype \"string\"\n",
    "df_weather['weather'] = df_weather.weather.astype('string')\n",
    "df_weather.weather.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data and Imputing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data is a common problem when working with real data sets. The reasons for missing data can be manifold, e.g. sensor failures, old data, improper data management and even human error. \n",
    "\n",
    "It is important that missing data is identified and dealt with appropriately before further data analysis or machine learning. Many machine learning algorithms cannot handle missing data and require that entire rows or columns where a single missing value is present be deleted. Or missing data has to be replaced (imputed) with a new value.\n",
    "\n",
    "Knowing when missing data occurs can help avoid missing values in the future, or help in dealing with them.\n",
    "There is a great python library for doing some EDA specifically for missing data, called **[missingno](https://github.com/ResidentMario/missingno)**. (Do not confuse it with the pokemon of the same name.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.pokewiki.de/images/2/28/Tokiya_Missingno.jpg\" alt=\"drawing\" style=\"width:250px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import missingno\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at how many values are missing per column in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display number of missing values per column\n",
    "df_weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With missingno we can simply display these values as missing percentages per column in a bar chart. Each bar represents one column. On the left side we see the percentage of data present, on the right side we see the exact number of values that correspond to these percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting percentage of missing values per column\n",
    "msno.bar(df_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only two columns contain missing values: More than 20 % of the data is missing in the column \"temp_max\". Less than 10 % of the data is missing in the column \"precipitation\". It would be interesting to know if there is a pattern in the occurrence of the missing values. This would help us understand where they are coming from. For example, it could be that these values are not collected on Sunday. Then we would have a missing value in every 7th row. It could also show us a period when our instrument stopped working, which would then lead to successive missing values. All this helps us to understand our data better and to correct errors if necessary.\n",
    "\n",
    "With missingno you can also print a matrix plot. For each row in the dataset you can see if data is missing in each column. When data is present, the plot is shaded in grey (or your color of choice), and when it is absent the plot is displayed in white. This gives you an view how that missing data is distributed in the data frame. And if they occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the right hand side you can see for each data point how many columns contain data. In our case, the range is from 4 columns with data (left) to 6 columns with data (right). As you can see, there are not many rows where missing values occur together. And overall, there is no clear pattern in the missing data. Therefore, we have to assume, that the missing data occurs randomly. Which is not helping us at all on how to best treat this missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missingno has more plots to investigate missing values. For example, you can also create a heatmap to see how often missing values in specific features occur together. Check out the repo for more insights!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Data and Imputing are big topics in data science. Imputing means to replace missing values with sensible values. There are several imputing techniques that we will familiarize ourselves with in this notebook. When you impute missing values, you are manipulating the data and this can have a significant impact on the quality of the data.  Hence you need to have a certain level of knowledge about your data in order to find a good solution and the approach that is best suited for the task at hand. As usual in data science, there is no one solution that fits all problems. Imputation of data ranges from fairly simple to highly complex strategies. Remember that the work of a data scientist progresses in cycles. It is always best to start with a simple approach and become more complex with each cycle. With this in mind, your pre-processing will only become more complex and thus more time-consuming if it really provides more value.\n",
    "\n",
    "The first step is always to determine how many missing data you got. Since we already identified all our missing data above, we only have to use one command.\n",
    "\n",
    "> Exercise: Time for brainstorming! What could be reasons for missing values and what different ways of dealing with missing data can you think of? Are these possibilities dependent on the reasons of missing values?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping data is the easiest and fastest option. But this is only recommended if there’s a lot of data to start with and the percentage of missing values is low. There are multiple ways to drop your data. You could drop the rows which have missing values in them or even drop a whole column.\n",
    "\n",
    "Though you could end up with little data if you have too much missing data. You also could loose some (valuable) information or maybe evan change the representation of some data if the rows with missing values have something in common and are not totally random. Hence you should always be mindful about dropping your data.\n",
    "\n",
    "In our case we have only 2 columns with missing values and the amount doesn't seem too much. But let's put those numbers in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"numbers of rows : {df_weather.shape[0]}\")\n",
    "print(f\"missing values in precipitation : {round(df_weather.precipitation.isna().sum()/df_weather.shape[0]*100,2)} %\")\n",
    "print(f\"missing values in temp_max : {round(df_weather.temp_max.isna().sum()/df_weather.shape[0]*100,2)} %\")\n",
    "print(f\"missing values in data frame : {round(df_weather.isna().sum().sum()/(df_weather.shape[0]*df_weather.shape[1])*100,2)} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1461 rows and about one quarter of them have missing values in at least one column. This would actual be too much to drop. Fortunately there are more ways of imputing. But for a start dropping is still a possibility.\n",
    "\n",
    "Pandas has a function for dropping missing values. You will find a description and an explanation for a few attributes in the following table:\n",
    "\n",
    "`dropna` | description\n",
    "---|---\n",
    "`.dropna()` | drops row if contains at least 1 NaN\n",
    "`.dropna(how='all')` | drops row if all entries are filled with NaN\n",
    "`.dropna(thresh=2)` | drops row if there are less than 2 cells filled with a valid value\n",
    "\n",
    "You can also checkout the [documentation for `dropna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).\n",
    "\n",
    "> **Exercise:** Since we also want to show you other ways of imputing in this notebook, make a copy of our dataset. After that use `.dropna` on our copied data set. Try at least two different ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the dataset\n",
    "\n",
    "\n",
    "#drop all rows if they contain at least one missing value (in any column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing with a constant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flag the missing value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes even a missing value can be informative. For example if you asking a proband for their sex and give them only the option of ticking female or male and their define them selfs as queer, it might conclude in a missing value. You could assume and assign that value to `other` or another categorical value.\n",
    "For our case we could assign a unique value. For precipitation and temperature this would be a really high absolute value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean/Median/Mode imputation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular technic of imputing is to take the mean, median or mode value of the column. Which one of those three to take depends on the issue you are dealing with. For example if you are having a categorical value (like sex: 0 for male, 1 for female), taking the mean value would not make any sense. Also taking the mean for a numerical value could be dangerous since outliers could have a huge impact. In that case taking the median could be a better choice.\n",
    "Of course it is also possible to take assumptions, for example the weather today is the same weather as the day before.\n",
    "\n",
    "> <sup>Hint: A visualization plot could help you determining which value to take.</sup>\n",
    "\n",
    "As you probably already guessed, pandas has an implemented function for the imputing issue:\n",
    "\n",
    "`fillna` | description\n",
    "---|---\n",
    "`df.fillna(0)` | replaces all nan in df with one value\n",
    "`df.fillna({'precipitation': 0, 'temp_min': 0, 'temp_max': 42, 'wind': 0, 'weather': 'no_weather')}` | replaces nan in specific column with one specific value\n",
    "`df.fillna(method='ffill')` | use value of the day (row) before (forward fill)\n",
    "`df.fillna(method='bfill')` | use value of the day (row) after (backward fill)\n",
    "`df.fillna(method='ffill', axis=1)` | copies values from column before\n",
    "`df.fillna(method='ffill', limit=1)` | if there are more than 1 value in a row missing, only the next one will be filled with the previous day value, the other one will stay as NaN\n",
    "\n",
    "You can also checkout the [documentation for `fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html).\n",
    "\n",
    "> **Exercise:** Please make a copy from `df_weather` again and fill your missing values. Try at least two methods with `.fillna`. Which values would make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the dataset\n",
    "\n",
    "\n",
    "# use fillna to impute missing values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final method we want to try out today is calculating the missing values with an interpolation. An interpolation estimates the course of a continuous function between two known points. From this function we can approximate the missing value.\n",
    "\n",
    "There are multiple ways of interpolation. The most common and also the easiest is the linear interpolation, where two data points, $(x_{i-1},y_{i-1})$ and $(x_{i+1},y_{i+1})$, are connected by a straight line.\n",
    "\n",
    "$$\n",
    "y_{i} = y_{i-1} + \\frac{y_{i+1} - y_{i-1}}{x_{i+1} - x_{i-1}}(x_i - x_{i-1})\n",
    "$$\n",
    "\n",
    "In our case the maximum temperature is a perfect example. If we use the default settings of pandas `.interpolate` we would get a linear interpolation with an equal spaced distribution. It is also possible to take the date instead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Please make a copy from `df_weather` again and try to do an interpolation for the missing values in the maximum temperature column. You might want to checkout the [documentation for `interpolate`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the dataset\n",
    "\n",
    "\n",
    "# interpolate the missing values in temp_max column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is also possible to use splines and polynomials, the interpolation function can be quit powerful. Reading further documentation is absolute necessary to use this function!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to predict your missing values. Therefor the column with the missing data would become the target value. The dataset will be split into a set with a valid value for the target which would be used for training and a set with the missing values, for which we would predict. A good model for imputing is KNN. Since we didn't reach the subject of predictive modeling yet, this will only be mentioned at this point.\n",
    "\n",
    "> **Note:** It is important to prevent any data leakage! When using a model to impute missing data make sure you remove your actual target values! If you impute temp_max, you cannot use the weather columns when your end goal is to predict the weather.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.data_cleaning': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9f476cfdb166f40c134cbcc8d5dc6c4ffea6e72dd31b817de73f4b973ffdff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
