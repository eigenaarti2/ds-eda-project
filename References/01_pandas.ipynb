{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to pandas "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Pandas_logo.svg/1920px-Pandas_logo.svg.png\" height=\"200\"/></center>\n",
    "\n",
    "Today we're going to be starting our journey through the pandas library (created by Wes McKinney). The name is derived from the term \"panel data\", an econometrics term for data sets that include observations over multiple time periods for the same individuals.\n",
    "\n",
    "When we refer to working in pandas, we're typically talking about working within a **DataFrame**, which is what we'll focus on today. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this notebook you will be able to:\n",
    "\n",
    "- explain why pandas is important to Data Scientists\n",
    "- create pandas Dataframes from lists or a list of dictionaries\n",
    "- load data from csv-files to DataFrames\n",
    "- access data stored in DataFrames\n",
    "- use the common features of a DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll be diving into pandas DataFrames, which are objects that will hold our data, allowing us to interact with it, manipulate it, and eventually throw it into machine learning algorithms (if we want). \n",
    "\n",
    "Since a pandas DataFrame is an **object**, this means that we're going to interact with it in much the same way that we interact with all of our other objects in python. Before we get to actually interacting with DataFrames, though, we'll have to get one, and get one with data in it! There's one quick step that we have to do before that... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Import "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd # Standard import. \n",
    "```\n",
    "\n",
    "Here I've shown how we get access to everything in the pandas library - we import it! Also note the python comment `\"# Standard import\"` out to the right of our import. This was to note that this is the standard way to import the pandas library. \n",
    "\n",
    "\n",
    "We should always be sure that if we are importing the entire pandas library, we follow this syntax. It's common practice to use `pd` as the alias, and we tend to follow common practice whenever possible. This makes it easier for others to read our code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a DataFrame Object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic ways that we can get a pandas DataFrame object to work with. The first is by using data that is already in our Python program, in conjunction with the `DataFrame` constructor. The second is by reading in external data through the pandas module (which we've imported and made accessible via `pd`). For reference, here are the [docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for pandas DataFrames. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using data already in our Python program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using data that is already in our Python program, then we are going to be passing that data to the `DataFrame` constructor. We typically do this in one of two ways. The first involves passing in a list of dictionaries, whereas the second involves passing in two lists. Let's dive into the first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # We haven't actually done this in code yet. \n",
    "data_lst = [{'a': 1, 'b': 2, 'c':3}, {'a': 4, 'b':5, 'c':6, 'd':7}]\n",
    "df = pd.DataFrame(data_lst) \n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames consist of **rows** and **columns**, where the columns will be denoted and accessed via their names, and the rows will be denoted and access via the indices of the DataFrame. Above, we can look at our columns and see that their names are `a`, `b`, `c`, and `d`. We can similarly look at our rows and see that they are indexed by `0` and `1`. These column names and indices are how we will access this data later. How did the `DataFrame` constructor take our list of dictionaries and put it into the DataFrame in that format, though?\n",
    "\n",
    "When the pandas DataFrame constructor encounters a list of dictionaries like we gave it, it interprets each dictionary to be a row in the DataFrame. The keys are read as the column names and the values as the values for each column. By default, the DataFrame constructor will assign a column for **every** key that it sees in **any** dictionary in the list of dictionaries. If a particular dictionary in that list doesn't have a value for that key, then it assigns a `NaN` (stands for \"not a number\") value for that index-column pair. Therefore, when the pandas DataFrame above got the list of dictionaries, it saw `a`, `b`, `c`, and `d` keys, and thus created those columns. It then filled in the values associated with those keys, filling in a `NaN` if it didn't find that key (like it didn't find `d` in the first dictionary in our list). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst = [{'a': 1}, {'b':5}, {'c': 4}]\n",
    "df = pd.DataFrame(data_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you expect our DataFrame to hold now?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way of creating a dataframe from data that is already in our Python program is to pass in a list of lists as the `data` argument, and a list of strings as the `columns` argument. The `pd.DataFrame()` constructor will assume that each individual list in the `data` argument is one row (i.e. if you pass in a list of 5 lists, your dataframe will have 5 rows). Below, we're passing in a list of 2 lists to the `data` parameter, which means that our DataFrame will have two rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vals = [[1, 2, 3], [4, 5, 6]]\n",
    "data_cols = ['a', 'b', 'c']\n",
    "df = pd.DataFrame(data=data_vals, columns=data_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that this method is not quite as flexible as using a list of dictionaries. When passing in a list of lists via the `data` argument, we have to make sure that the greatest number of elements in any single list corresponds to the number of column names we are passing in via the `columns` argument (no more, no less). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vals = [[1, 2], [4, 5, 6]]\n",
    "data_cols = ['a', 'b']\n",
    "df = pd.DataFrame(data=data_vals, columns=data_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do, however, have the flexibility of passing in some rows with 'missing' data. The key to note here, though, is that the last column name will become what is filled with a `NaN` if a list is missing a column value (it will not be based off a key name like in our list of dictionaries). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vals = [[1, 2], [4, 5, 6]]\n",
    "data_cols = ['a', 'b', 'c']\n",
    "df = pd.DataFrame(data=data_vals, columns=data_cols)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading External Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways that we can read external data into a pandas DataFrame, and they will be called as a function that is available via the `pandas` module. \n",
    "\n",
    "\n",
    "As a result of importing the `pandas` module and making it accessible via `pd`, this means that we will call all of these functions via `pd`. Each one of these functions will **return** back to us a pandas DataFrame object, populated with the external data that we read in. \n",
    "\n",
    "The [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) will show you all of the ways that you could load external data into a DataFrame. Basically, there is a way to load in data stored in any format (CSV, JSON, SQL, Excel, HTML). All of these take some form of a `read_{data_type}` function, which means that we will call them as `pd.read_{data_type}`.\n",
    "\n",
    "So, if we wanted to load data in from a CSV, we would simply use:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('my_data.csv')\n",
    "```\n",
    "\n",
    "Note: This assumes that we have the column names in the first row of your .csv.\n",
    "\n",
    "If we don't have the column names in the first row of our .csv, we could read in the .csv with the following:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('my_data.csv', header=None)\n",
    "```\n",
    "\n",
    "Note: This by default assigns numbers as column names (starting with 0).\n",
    "\n",
    "If we wanted to assign the column names as we read it in, we can pass in an additional `names` argument, where this `names` argument holds a list of the names we want to assign to the columns.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('my_data.csv', header=None, names=['col1', 'col2', ...., 'col12'])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Our Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is available in the `data` folder inside this repo. It's called `abalone.csv`. We will first read in the data, and then discuss what it's all about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/abalone.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our `DataFrame` is an object, we can imagine that it will have associated attributes and methods. There are a couple of each available to us to get a general sense of our data. We have two attributes that we will frequently use on our DataFrame - these will allow us to look at the shape of our data and the column names. We have four methods that are available on our dataframe for getting a general sense of our data: `info()`, `describe()`, `head()`, and `tail()`. Let's take a look at what these do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which this data we are taking a trip under the sea...\n",
    "\n",
    "The data is all about abalones, which are very large sea snails.\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/bc/Abalone_at_California_Academy_of_Sciences.JPG\" height=\"500\"/></center>\n",
    "\n",
    "The data is derived from UCI Machine Learning Repository for open data sets. You can find [more information](https://archive.ics.uci.edu/ml/datasets/Abalone) at the UCI page.\n",
    "\n",
    "The original goal is the find out the age of those sea snails through other easily obtainable physical measurements, without having to cutting them off and count the number of rings, which is pretty time-consuming.\n",
    "Below you see a table containing all attributes of the snails that have been examined. You found those in as column names in our newly created DataFrame as well.\n",
    "\n",
    "\n",
    "**Column Description:**\n",
    "\n",
    "Name |  Data Type | Measurement Unit | Description\n",
    ":-----|--------|--------|:--------\n",
    "Sex | nominal | -- | M, F, and I (infant)\n",
    "Length | continuous | mm | Longest shell measurement\n",
    "Diameter | continuous | mm | perpendicular to length\n",
    "Height | continuous | mm | with meat in shell\n",
    "Whole weight | continuous | grams | whole abalone\n",
    "Shucked weight | continuous | grams | weight of meat\n",
    "Viscera weight | continuous | grams | gut weight (after bleeding)\n",
    "Shell weight | continuous | grams | after being dried\n",
    "Rings | integer | -- | +1.5 gives the age in years\n",
    "\n",
    "We already saw the first rows of our DataFrame, when we executed the command `df.head()`. By default this command shows us 5 rows, but we can alter it to any number of rows we want to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(8) # or n=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can check out the last rows in our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two attributes of our DataFrame are highly useful for us to look at:\n",
    "`shape` and `columns`.\n",
    "\n",
    "The `shape` will return us the number of rows and columns of our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives us back a list of all of the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that we have 4177 individual observations (in this case individual sea snails). In terms of machine learning, we have 8 different features to predict the desired response (output), which in this case are the `n_rings` used to evaluate the age of the sea snails.\n",
    "\n",
    "In the column description above, the data types of each column is stated. To check whether the data was loaded correctly and the right data types were detected, we can have a look which data types were connected to the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data was loaded in correctly. To get a feeling of the data, it is highly recommended to get some statistics (better even, a summary of them). This helps us to evaluate the spread and central tendency of out data.\n",
    "\n",
    "Of course also for this task, there is a pretty neat function already implemented in pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see, that in this summary, the column `sex`is missing. `.describe()`will only show us numeric columns.\n",
    "The first row shows us how many observations are available for each feature. The same information was also stored at `.info()`. Luckily in our case, we have no missing data, as the count equals the the # of rows in our DataFrame. This would be different if we would have any `nan` in our DataFrame.\n",
    "\n",
    "The other values: Mean, Standard deviation, min, max and the quartiles should be familiar to all of you already."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing your data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now know how to look at our data. What if we wanted to grab certain parts to look at, or certain parts to play around with/transform? Say we wanted to grab an entire row, or an entire column... how do we do that? Let's dive in by starting off with some indexing. \n",
    "\n",
    "The format we use to index into our dataframe and grab data will depend on exactly what subset of the data that we want to grab. If we want to grab entire rows or columns, then we can use bracket notation to do that (just like we use bracket notation to index into lists). If we want an entire column, then we're going to place the **column name** in brackets (and multiple column names in a list inside those brackets). We can also sometimes access a column via dot notation on the dataframe, which we'll show in a second. If we want an entire row, then we have to place **both** a **beginning and ending index** inside the brackets (it won't work to just place a single index in the brackets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs the \"sex\" column: bracket notation\n",
    "df['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs the \"sex\" column: dot notation\n",
    "df.sex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: The dot notation won't work if the column names contain spaces! That's why it is convenient to check first, if column names contain them and change them into \"_\" (underscored).\n",
    "\n",
    "Luckily, the columns in our DataFrame have already underscores.\n",
    "If not, here is a short code snippet, which you could use to change the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of your dataframe\n",
    "df2 = df.copy()\n",
    "# make a list column names\n",
    "cols = df2.columns.tolist()\n",
    "# replace space with _\n",
    "cols = [col.replace(' ', '_') for col in cols]\n",
    "# reassign new column names to dataframe\n",
    "df2.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access multiple columns by passing in a list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sex', 'length']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will grab from the beginning up to but not including the row at index 3. \n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will not work because we didn't give it a starting **and** ending index.\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't work because we are trying to access a subset of rows \n",
    "# **and** columns at the same time. \n",
    "df[:1, 'volatile acidity'] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to grab certain rows **and** certain columns, rather than just entire rows or entire columns?\n",
    "\n",
    "If we want to grab only certain rows and columns, there two **methods** that we can use to index into a pandas DataFrame: `loc[]` and `iloc[]`. Note that these are **methods**, which means that we will call them via dot notation on our `DataFrame` object. The difference between these three has to do with how we use them. `loc[]` is a purely label-location based indexer, `iloc[]` is a purely integer-location based indexer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loc is label based. All of these will work, because they are recognized as labels on the \n",
    "# rows (index labels) or columns (column name labels). \n",
    "df.loc[0, 'sex'] # 0 is one of the index labels, and 'sex' is a column label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranges on our index labels still work (as long as they're numeric).\n",
    "df.loc[0:10, 'sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[10:15, ['sex', 'length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will all fail, because they attempt to access the columns by position integers, \n",
    "# and loc only takes labels. \n",
    "df.loc[0, 0]\n",
    "df.loc[0:10, 0]\n",
    "df.loc[10:15, [0, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above will all work with .iloc, though, since it takes integers (and not labels)\n",
    "df.iloc[0, 0]\n",
    "df.iloc[0:10, 0]\n",
    "df.iloc[10:15, [0, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using labels, though, like we did with .loc, will NOT work.\n",
    "df.iloc[0, 'sex']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little bit more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to grab certain rows or columns from dataframes, as well as a subset of rows and columns, and anything in between. But, this looks like it typically requires that the exact location of the data we want is known. What if we don't know that location? Is there a way to grab desired data by simply specifying some query parameters? Yes! \n",
    "\n",
    "There are a couple of ways that we can do this. The first way we'll look at is just through masking, whereas the second actually uses the `query()` method available on the pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just gives us a mask - tells us True or False whether each row fit's the condition.\n",
    "df['length'] <= 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use a mask, we actually have to use it to index into the DataFrame (using square brackets). \n",
    "df[df['length'] <= 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how only the indices that were found to be True from the condition show up in this subset of the dataframe. We've \"masked\" off the rest of the indices that we're found to be False (hence the name **masking**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, this is cool. What if I wanted a slightly more complicated query...\n",
    "df[(df['length'] <= 0.4) & (df['sex'] == 'F')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I could write an arbitrarily complicated query using that syntax... \n",
    "df[(df['length'] <= 0.4) & (df['length'] > 0.35) & (df['weight_whole'] > 0.25) & (df['weight_whole'] < 0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or I could use the query() method that is available on our dataframe object. \n",
    "df.query('length <= 0.4 and length > 0.35 and weight_whole > 0.25 and weight_whole < 0.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it's preferred to use the `query()` method, since it improves readability. It doesn't use loads of sets of brackets (`[]`) and parentheses (`()`), but rather just one set of parentheses. It also tends to follow the Python syntax a little more closely than the mask methods that we looked at above, using `and` instead of `&` to separate different specifications on our queries. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We're going to look at a couple of more things you can do with DataFrames, but to view all available attributes and methods of DataFrames, we can check out the [pandas Docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). For practical examples of how DataFrames are used you can get a copy of [Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do) (it's written by Wes McKinney, the creator of pandas).\n",
    "\n",
    "For those coming from R, know that the pandas DataFrame was based off the R DataFrame, and most anything we can do with an R DataFrame, we can do with a pandas DataFrame. For anybody coming from a SQL background, the methods available via DataFrame's give us much, if not all, of the functionality that we have available in your SQL environment. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with `groupby`s...\n",
    "\n",
    "By “group by” we are referring to a process involving one or more of the following steps\n",
    "\n",
    "- __Splitting__ the data into groups based on some criteria\n",
    "- __Applying__ a function to each group independently\n",
    "- __Combining__ the results into a data structure\n",
    "\n",
    "[source](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we try to groupby the sex, let's check how many different values this feature can have \n",
    "df['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also see the count of different values with this command\n",
    "df['sex'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sex')   # Note that this returns back to us a groupby object. It doesn't actually \n",
    "                    # return to us anything useful until we perform some aggregation on it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tons of aggregation metrics we can get from a groupby object. Note here that we store the results of a groupby below to then perform all kinds of operations on it (this is actually the preferred method if we're going to perform more than one calculation on it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by sex column\n",
    "groupby_obj = df.groupby('sex')\n",
    "\n",
    "# Performing an aggregation by calculating the mean\n",
    "groupby_obj.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_obj.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_obj.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous aggregation metrics gave us back a DataFrame with all of the columns minus what we grouped on. Notice that what we grouped on becomes the index. What if I wanted only one column back (especially with something like count, where it is the same for every column)? \n",
    "Well, we can do anything with this DataFrame that we did before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sex').count()['length']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we can also group by multiple columns by passing them in in a list. It will group by the first column passed in first, and then the second after that (i.e. it will group by the second within the group by of the first). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "df.groupby(['sex', 'n_rings']).count()['weight_whole']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the [Group By documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) to look at what all you can do with the pandas `.groupby()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sorting is going to work much the same way as a group by. It is going to be available via a method that we call on the dataframe, `.sort_values()`, and we are going to pass it a column or columns to sort by. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('length').head(15) # Note: this is ascending by default.\n",
    "# df.sort_values('length', ascending=False) # here we change ascending to false."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort by multiple columns by placing them in a list inside of the sort() method. It will sort by the first column passed in first, and then the second within the sort of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by multiple columns\n",
    "df.sort_values(['length', 'diameter'], ascending=False).head(15) # ascending=False will apply to both columns. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and Dropping Columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating columns is done in one of two ways: \n",
    "1. Using bracket notation\n",
    "2. Using the `eval()` method on the pandas DataFrame. \n",
    "\n",
    "Dropping columns is done using the `df.drop()` method on the pandas DataFrame. When dropping columns, we have to be careful to make sure to tell the DataFrame to drop them in place, or assign the DataFrame with dropped columns to a new variable. You also need to make sure to tell the `drop()` method what axis the thing you're trying to drop is on (rows are `axis=0`, and columns are `axis=1`).\n",
    "![](https://i.stack.imgur.com/DL0iQ.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can rename columns with `.rename()``\n",
    "df.rename(columns={'n_rings': 'nr_rings'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with `.eval()` we can create new columns\n",
    "# remember that in the description it is stated that the age of the snails can be calculated by adding 1.5 to the nr. of rings\n",
    "df.eval('age = nr_rings + 1.5', inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.eval('weight_per_hight = weight_whole / diameter', inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is also necessary to delete columns, e.g. if they do not contain relevant data, or if engineered features contain the same information.\n",
    "\n",
    "The age is our newly created feature, therefore nr_rings are not containing any further information with axis=1 we define that we are dropping a column, not a row by default it would be axis=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nr_rings column\n",
    "df.drop('nr_rings', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # Wait, the nr_rings column is still there... why? \n",
    "           # It's because we didn't tell it drop inplace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('nr_rings', inplace=True, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Nulls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has functions for both filling nulls (or N/As) with whatever value we want, or dropping nulls all together. To fill nulls, we use the `.fillna()` method on the DataFrame, and to drop nulls, we call the `.dropna()` method on the DataFrame. In terms of the `.fillna()` function, we can give it a default value to fill in, or a number of other methods to fill it in (padding, back filling, forward filling). You can read about dealing with missing data in the docs [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html). We're not going to go into too much depth here, but want you to know that this functionality exists. \n",
    "\n",
    "As we already checked, in this dataset, there are not even missing data. So the next line of code will not do anything in this case. Just so you know how to fill nans or drop them with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the addition of the inplace argument here. \n",
    "# if you try to fill nans will some value: the first argument is the inserted value\n",
    "df.fillna(-1, inplace=True)\n",
    "# dropna() will drop the whole row, if nans are present\n",
    "df.dropna(inplace=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick aside on Pandas Series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that in a couple of places, when we asked for certain rows/columns of the data, we got back a 1-D array that had an index attached. These are examples of what pandas calls `Series`. In the documentation for [pandas Series](http://pandas.pydata.org/pandas-docs/version/0.15.2/dsintro.html#series), you can get an idea of what they can do. For the most part, we can kind of treat them like a mini DataFrame, as they have a lot of the same methods. However, there are some slight differences. Since we work with DataFrame's the majority of the time, we're not going to go into any real depth on Series. \n",
    "\n",
    "Here are some examples of things that returned series: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] <= 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['length'] <= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sex').count()['age']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of input! Don't worry, you can test and solidify your knowledge in the next notebooks.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas' DataFrame is the de facto data structure for Data Scientists\n",
    "- Get your data into a DataFrames ASAP\n",
    "- Use built-in methods to go fast!\n",
    "    - For example, use groupby to calculate aggregate statistics for categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Further Study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more functions on DataFrame you can discover. Here we have put together a few more links that describe the functionality of other functions and have great code snippets that you can use in your next projects.\n",
    "\n",
    "- [Code snippets on data handling and machine learning with python](http://chrisalbon.com) \n",
    "- [Book - Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.pandas-numpy': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "74c6cb286e0ae01e6e36d62f264b80149270f9e729c4f678639691c812c01bcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
